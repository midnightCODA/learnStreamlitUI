# -*- coding: utf-8 -*-
"""Langchain in 15 minutes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j7v6WeA3xk8WmmuIeMabZx27Ac7WIY7O
"""


!pip3 install chainlit
!pip3 install langchain[llms]
!pip3 install openai
!pip3 install langchain-community
!pip3 install langchain
!pip install wikipedia
!pip install google-search-results
!pip install google-search-results
!pip3 install unstructured
!pip3 install python-magic-bin
!pip3 install chromadb
!pip3 install tiktoken
!pip install faiss-cpu










# !pip install faiss-gpu cant afford this yet

from langchain.agents import AgentType, initialize_agent, load_tools
import json

from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

import os
os.environ['OPENAI_API_KEY'] = "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"
os.environ['SERPAPI_API_KEY'] = "e1ae793a2b03140d73f9efb132ad50d0569b02cb9ffa7a85b8a88da2fcc6e8ca"


from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)
tool = WikipediaQueryRun(api_wrapper=api_wrapper)

tool.run('life skills')


"""### Youtube search module"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  youtube_search

from langchain.tools import YouTubeSearchTool
tool = YouTubeSearchTool()
tool.run("life skills")

from langchain.llms import OpenAI

llm = OpenAI(temperature=0.1)

# the tools lets load em
tools = load_tools(["serpapi", "llm-math"], llm=llm)

# Assuming initialize_agent returns an AgentExecutor
agent_executor = initialize_agent(tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# Set handle_parsing_errors to True
agent_executor.handle_parsing_errors = True

# name = llm.predict("What is langchain?")
# print(name)
agent_executor.run("what is 10 plus 10?")

"""### langchain Documentation, example test

"""

import os

os.environ["GOOGLE_CSE_ID"] = "17518b3ef854545ef"
os.environ["GOOGLE_API_KEY"] = "AIzaSyA11vh3A9izSeFZrAUR1DXJbPkAk1EzoXk"

from langchain.tools import Tool
from langchain_community.utilities import GoogleSearchAPIWrapper

search = GoogleSearchAPIWrapper()

tool = Tool(
    name="Google Search",
    description="Search Google for recent results.",
    # memory=ConversationBufferWindowMemory(k=2),
    func=search.run,
)

tool.run("is it found in dar es salaam?")

"""### Break through in Serpapi..."""


from langchain_community.utilities import SerpAPIWrapper

search = SerpAPIWrapper()

search.run("Tanzania GDP in 2023")

"""try to make serpapi work by pip install google-search-results

### Train GPT it with your OWN DATA
"""



from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import OpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import DirectoryLoader
import magic
import os
import nltk

"""neccesary installs"""



# Get your loader ready
loader = DirectoryLoader('/content/drive/MyDrive/FYP_Training_data', glob='**/*.txt')
documents = loader.load()
documents

# Get your text splitter ready
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

# Split your documents into texts
texts = text_splitter.split_documents(documents)

# Turn your texts into embeddings
embeddings = OpenAIEmbeddings()

# Get your docsearch ready
docsearch = FAISS.from_documents(texts, embeddings)

# Load up your LLM
llm = OpenAI()

# Create your Retriever
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=docsearch.as_retriever())

# Run a query
query = "ask questions about life skills"
qa.run(query)

"""### HOW CAN WE TEACH ABOUT LIFESKILLS with one paragrapgh at a time"""

query = "teach me about life skills in 10 paragraphs"
qa.run(query)

"""### Memory in our app"""

# Try to add follow up questions on the previous points.... i.e lets implement memory

from langchain.memory import ConversationBufferWindowMemory

from langchain.chains import ConversationChain
conversation_with_summary = ConversationChain(
    llm=OpenAI(temperature=0),
    # We set a low k=2, to only keep the last 2 interactions in memory
    memory=ConversationBufferWindowMemory(k=3),
    verbose=True
)
conversation_with_summary.predict(input="how can you run that fast?")